################################################################################
#                                                                              #
# This Script scraps webpages and chek if it's a parking page or a real        #
# website                                                                      #
#                                                                              #
################################################################################


#!/bin/python3

### Import ###
# Original
from bs4 import BeautifulSoup
import concurrent.futures
import logging
import logging.handlers as handlers
import urllib3
import requests
import socket
import sys
import threading
import tldextract
# Perso
# sys.path.append('/root/Scripts/Reco')
sys.path.append('/home/the-freeman/Scripts/Reco/')
from File import file_library as file_lib


### Log ###
# logging.basicConfig(filename='Log/web_scrapping_final.log', \
#                     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \
#                     level=logging.INFO)
# logger = logging.getLogger('web_scrapping_final')
# logger.setLevel(logging.INFO)
# # logger.setLevel(logging.INFO)
# # Here we define our formatter
# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# # Defining a file rotation for logs
# logHandler = handlers.RotatingFileHandler('Log/info.log', maxBytes=150000, backupCount=1)
# logHandler.setLevel(logging.INFO)
# logHandler.setFormatter(formatter)
#
# logHandler = handlers.RotatingFileHandler('Log/debug.log', maxBytes=250000, backupCount=1)
# logHandler.setLevel(logging.DEBUG)
# logHandler.setFormatter(formatter)
#
# errorLogHandler = handlers.RotatingFileHandler('Log/error.log', maxBytes=15000, backupCount=1)
# errorLogHandler.setLevel(logging.ERROR)
# errorLogHandler.setFormatter(formatter)
#
# logger.addHandler(logHandler)
# logger.addHandler(errorLogHandler)
### Log ###


#################################################################################
#                                                                               #
# Function: Scrap an URL and return the number of code's lines                  #
# Parameter: FQDN (String)                                                       #
# Return: The number of lines in the HTML code. It returns 0 if no website was  #
# found (int)                                                                   #
#                                                                               #
#################################################################################
def scrap_website(fqdn):
    ## Variables
    adapter = requests.adapters.HTTPAdapter(max_retries=0)
    resp_res = {'lines': 0, 'resp_code': 0, 'port': 'N/A', 'history': [], 'final_url': ''}
    session = requests.Session()
    url01 = 'https://' + fqdn
    url02 = 'http://' + fqdn

    # Prepares a special HTTP request with a specific adapter
    session.mount(url01, adapter)

    # Send an hTTPS request
    try:
        resp = session.get(url01, verify=False, timeout=5)
        resp_soup = BeautifulSoup(resp.text,"html.parser")

    except (requests.ConnectionError, requests.HTTPError, requests.Timeout,
            requests.TooManyRedirects) as e: # HTMLParser.HTMLParseError
        logging.error('scrap_website - HTTPS connection failed')
        logging.error(e)
        # print('lines: ' + resp)
        # If HTTPS failed, let's try HTTP with a 0.5s timeout (no SSL handshake to do)
        try:
            resp = session.get(url02, verify=False, timeout=0.5)
            resp_soup = BeautifulSoup(resp.text,"html.parser")

        except (requests.ConnectionError, requests.HTTPError, requests.Timeout,
                requests.TooManyRedirects) as e: # HTMLParser.HTMLParseError
            logging.error('scrap_website - HTTP connection failed')
            logging.error(e)
            resp_res['lines'] = 'NA'
            resp_res['resp_code'] = 'NA'
            resp_res['port'] = 'NA'
            resp_res['history'] = 'NA'
            resp_res['final_url'] = fqdn

        else:
            resp_res['lines'] = len(str.splitlines(str(resp_soup.prettify())))
            resp_res['resp_code'] = str(resp.status_code)
            resp_res['port'] = 'HTTP'
            resp_res['history'] = resp.history
            resp_res['final_url'] = resp.url
            logging.debug('scrap_website - HTTP connection successful')
            logging.debug(resp.url)

    else:
        resp_res['lines'] = len(str.splitlines(str(resp_soup.prettify())))
        resp_res['resp_code'] = str(resp.status_code)
        resp_res['port'] = 'HTTPS'
        resp_res['history'] = resp.history
        resp_res['final_url'] = resp.url
        # resp_a_list = resp_soup.find_all('a')
        logging.debug('scrap_website - HTTPS connection successful')
        logging.debug(resp.url)

    return resp_res


### Main Program ###
if __name__ == '__main__':
    ## Variables
    assets = {'WEB': [], '_WEB': [], '_REDIRECT': [], 'UNKNOWN': [], '400': [], '500': []}
    fqdn_list = []
    fqdn_statuscode_dict = {}
    thread_local = threading.local()
    # File
    fqdn_file = 'Data/fqdn_list.txt'
    result_file = 'Result/fqdn_statuscode_list.txt'

    # Get user input: File Name
    # file_name = str(sys.argv[1])

    # Disable unwanted SSL warnings
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    fqdn_list = file_lib.read_file(fqdn_file)

    # for url in url_list:
    #     scrap_website(url)
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        result = executor.map(scrap_website, fqdn_list)
        site_answer = list(result)

    for dict in site_answer:
        fqdn_statuscode_dict[dict['final_url']] = dict['resp_code']

    print(fqdn_statuscode_dict)

    file_lib.write_dict_to_file(result_file, fqdn_statuscode_dict)









    # Space
